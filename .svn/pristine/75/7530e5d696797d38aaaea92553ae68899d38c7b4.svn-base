package com.tudou.core.zeus.service

import java.io.File

import com.tudou.core.zeus.cms._
import com.tudou.core.zeus.common.hdfs.HdfsUtils
import com.tudou.core.zeus.common.dao.SortDao.{SortTaskRow, SortJobRow}
import com.tudou.core.zeus.common.dao.SortDaoUtils
import com.tudou.core.zeus.common.spark.SparkUtils
import org.apache.spark.sql._
import scala.collection.mutable.Map

/**
 * Created by wanganqing on 2015/3/11.
 */
class TaskManage(val task: SortTaskRow) {
  var jobRDDMap: Map[String, SchemaRDD] = Map[String, SchemaRDD]()
  var fullDataRDD: SchemaRDD = null
  var updateDataRDD: SchemaRDD = null

  def startSortTask(): Unit = {
    System.out.println("startSortTask")

    //读取全量和增量数据
    fullDataRDD = SparkUtils.createSchemaRDDByJsonFile(task.fullDataPath.getOrElse(""))
    //    updateDataRDD = SparkUtils.createSchemaRDDByParquetFile(task.updateDataPath.getOrElse(""))
    updateDataRDD = SparkUtils.createSchemaRDDByJsonFile(task.updateDataPath.getOrElse(""))
    //批量启动排序作业
    startAllSortJobByTask()
  }

  def startAllSortJobByTask() = {
    System.out.println("startAllSortJobByTask")
    val sortJobs: List[SortJobRow] = SortDaoUtils.getSortJobByTaskId(task.id)
    sortJobs.foreach(startSortJob)
  }

  def startSortJob(job: SortJobRow) = {
    System.out.println("startSortJob"+job)
    val key = job.key
    val rdd = SparkUtils.createSortJobSchemaRDD(fullDataRDD, updateDataRDD)(job.sortFileds).limit(task.defaultCacheMax.getOrElse(5000))
    jobRDDMap += (key -> rdd)
  }

  /**
   * 请求执行所有的排序作业
   */
  def runRequestAllSortJobByTask() = {
    val sortJobs: List[SortJobRow] = SortDaoUtils.getSortJobByTaskId(task.id)
    sortJobs.map(job => {
      new SortRequest(task.name, job.sortFileds.split(","), 0, task.defaultCacheMax.get)
    }).foreach(requestSortJob)
  }

  def requestSortJob(request: SortRequest) = {
    System.out.println("request"+request)
    if (!jobRDDMap.contains(request.key)) {
      val jobId = SortDaoUtils.insertSortJob(task.id, request)
      val sortJob = SortDaoUtils.getSortJob(jobId)
      startSortJob(sortJob)
    }
    val key = request.key
    val result = SparkUtils.take(jobRDDMap(key), task.filedIdNames.split(","), task.defaultCacheMax.getOrElse(5000))
    System.out.println("spark take count "+result.length)
    val result2 = result.map(row => row.mkString("_"))
    val sortResponse = new SortResponse(request, result2) //[id_id2_id_3,id_id2_id3]
    //发送到kafka消息
    KafkaUtils.send(sortResponse)
  }

  def updateData(request: DataRequest) = {
    request.isFullUpdate.booleanValue() match {
      case true => {
        updateData_Full(request)
      }
      case false => {
        updateData_update(request)
      }
    }
  }


  /**
   * 全量更新
   * @param request
   */
  def updateData_Full(request: DataRequest) = {
    val fullDataPath: String = request.data(0)
    //保存task.fullDataPath=newPath , task.updateDataPath = null
    SortDaoUtils.updateSortTaskFullDataPath(task.id, fullDataPath)
    SortDaoUtils.updateSortTaskUpdateDataPath(task.id, null)
    //替换旧的fullRDD
    fullDataRDD = SparkUtils.createSchemaRDDByJsonFile(fullDataPath)
    updateDataRDD = null
    //启动所有的排序作业
    startAllSortJobByTask()
    //请求执行所有的排序作业
    runRequestAllSortJobByTask()
  }

  /**
   * 增量更新
   * @param request
   */
  def updateData_update(request: DataRequest) = {
    //初始化需要更新的NewRDD
    val neadUpdateDataNewRDD = SparkUtils.createSchemaRDDByJson(request.data)
    //数据合并
    val newUpdateDataRDD = SparkUtils.mergeNewRDD(updateDataRDD, neadUpdateDataNewRDD, task.filedIdNames)
    //保存
    val updateDataPath = HdfsUtils.createTaskUpdateDataPath(task)
    newUpdateDataRDD.toJSON.saveAsTextFile(updateDataPath)
    //    newUpdateDataRDD.saveAsParquetFile(updateDataPath)
    //更新 task.updateDataPath = newPath
    SortDaoUtils.updateSortTaskUpdateDataPath(task.id, updateDataPath)
    //替换旧的updateRDD
    updateDataRDD = SparkUtils.createSchemaRDDByJsonFile(task.updateDataPath.get)
    //    updateDataRDD = SparkUtils.createSchemaRDDByParquetFile(task.updateDataPath.get)
    //启动所有的排序作业
    startAllSortJobByTask()
    //请求执行所有的排序作业
    runRequestAllSortJobByTask()
  }

}
